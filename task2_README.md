Creating a comprehensive README file is crucial for effectively communicating the purpose, usage, and details of your project. Here's a template for your project focused on fraud detection with logistic regression and SMOTE:

---

# Fraud Detection with Logistic Regression and SMOTE

## Project Overview

This project aims to develop a fraud detection system using logistic regression, with a focus on handling imbalanced datasets through the use of Synthetic Minority Over-sampling Technique (SMOTE). The project processes data in chunks, performs feature engineering, scales numerical features, one-hot encodes categorical features, and builds a predictive model to classify transactions as fraudulent or non-fraudulent.

## Table of Contents

- [Project Overview](#project-overview)
- [Table of Contents](#table-of-contents)
- [Dataset](#dataset)
- [Installation](#installation)
- [Usage](#usage)
- [Feature Engineering](#feature-engineering)
- [Handling Imbalanced Data](#handling-imbalanced-data)
- [Model Training and Evaluation](#model-training-and-evaluation)
- [Results](#results)
- [Contributors](#contributors)
- [License](#license)

## Dataset

The dataset contains transaction records, with the following features:

- `trans_date_trans_time`: Transaction date and time
- `cc_num`: Credit card number
- `merchant`: Merchant name
- `category`: Merchant category
- `amt`: Transaction amount
- `first`: Cardholder's first name
- `last`: Cardholder's last name
- `gender`: Cardholder's gender
- `street`: Cardholder's street
- `city`: Cardholder's city
- `state`: Cardholder's state
- `zip`: Cardholder's zip code
- `lat`: Cardholder's latitude
- `long`: Cardholder's longitude
- `city_pop`: Population of the city
- `job`: Cardholder's job
- `dob`: Cardholder's date of birth
- `trans_num`: Transaction number
- `unix_time`: Unix timestamp of the transaction
- `merch_lat`: Merchant's latitude
- `merch_long`: Merchant's longitude
- `is_fraud`: Target variable indicating if the transaction is fraudulent (1) or not (0)

## Installation

To run this project, ensure you have Python installed, and install the required libraries using:

```bash
pip install pandas scikit-learn scipy imbalanced-learn seaborn matplotlib lightgbm
```

## Usage

### Data Preprocessing

The script processes data in chunks, performs feature engineering, scales numerical features, and encodes categorical features.

### Feature Engineering

The following new features are created:
- `trans_hour`: Hour of the transaction
- `trans_day_of_week`: Day of the week when the transaction occurred

### Handling Imbalanced Data

The SMOTE technique is used to oversample the minority class (fraudulent transactions) to balance the dataset.

### Model Training and Evaluation

A Logistic Regression model is trained on the resampled data, and its performance is evaluated using classification metrics and the ROC AUC score.

### Example Script

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from scipy.sparse import csr_matrix, vstack, hstack
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter

# Function to process data in chunks and convert to sparse matrix
def process_data_chunks(file_path, chunk_size, scaler, encoder):
    chunks = pd.read_csv(file_path, chunksize=chunk_size)
    sparse_chunks = []
    
    for chunk in chunks:
        chunk['trans_hour'] = pd.to_datetime(chunk['trans_date_trans_time'], dayfirst=True).dt.hour
        chunk['trans_day_of_week'] = pd.to_datetime(chunk['trans_date_trans_time'], dayfirst=True).dt.dayofweek
        
        numerical_features = ['amt', 'lat', 'long', 'city_pop']
        chunk[numerical_features] = scaler.transform(chunk[numerical_features])
        
        categorical_features = ['merchant', 'category', 'gender']
        encoded_features = encoder.transform(chunk[categorical_features])
        
        sparse_data = csr_matrix(chunk[numerical_features])
        combined_data = hstack([sparse_data, encoded_features])
        sparse_chunks.append(combined_data)
    
    final_sparse_matrix = vstack(sparse_chunks)
    
    return final_sparse_matrix

initial_chunk = pd.read_csv('fraudtrain.csv', nrows=100000)
initial_chunk['trans_hour'] = pd.to_datetime(initial_chunk['trans_date_trans_time'], dayfirst=True).dt.hour
initial_chunk['trans_day_of_week'] = pd.to_datetime(initial_chunk['trans_date_trans_time'], dayfirst=True).dt.dayofweek

numerical_features = ['amt', 'lat', 'long', 'city_pop']
categorical_features = ['merchant', 'category', 'gender']
scaler = StandardScaler()
encoder = OneHotEncoder(handle_unknown='ignore', sparse=True)
scaler.fit(initial_chunk[numerical_features])
encoder.fit(initial_chunk[categorical_features])

file_path = 'fraudtrain.csv'
chunk_size = 100000
final_sparse_matrix = process_data_chunks(file_path, chunk_size, scaler, encoder)

data = pd.read_csv('fraudtrain.csv', usecols=['is_fraud'])
y = data['is_fraud']

X_train, X_test, y_train, y_test = train_test_split(final_sparse_matrix, y, test_size=0.2, random_state=42)

print("Original training dataset shape:", Counter(y_train))
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
print("Resampled training dataset shape:", Counter(y_train_res))

model = LogisticRegression(random_state=42)
model.fit(X_train_res, y_train_res)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))
print(f'ROC AUC Score: {roc_auc_score(y_test, y_pred)}')

conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
```

## Results

- **Classification Report**: Includes precision, recall, f1-score for each class.
- **ROC AUC Score**: Evaluates the overall performance of the classifier.
- **Confusion Matrix**: Visual representation of the classifier's performance.

## Contributors

- [MATHIVANAN S](https://github.com/smadhiva)

